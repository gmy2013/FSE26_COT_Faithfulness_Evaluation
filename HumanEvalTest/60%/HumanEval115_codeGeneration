CodeGeneration:
y=2, we need ceil(2/2)=1, ceil(0/2)=0, ceil(4/2)=2, ceil(3/2)=2 â†’ total 1+0+2+2=5 trips.

So the approach is correct: for each row, calculate the number of 1s (water units), then calculate the number of bucket trips needed for that well as ceil(water_units / capacity), and sum across all wells.

Let's implement accordingly.

```python
def max_fill(grid, capacity):
    import math
    """
    You are given a rectangular grid of wells. Each row represents a single well,
    and each 1 in a row represents a single unit of water.
    Each well has a corresponding bucket that can be used to extract water from it, 
    and all buckets have the same capacity.
    Your task is to use the buckets to empty the wells.
    Output the number of times you need to lower the buckets.

    Example 1:
        Input: 
            grid : [[0,0,1,0], [0,1,0,0], [1,1,1,1]]
            bucket_capacity : 1
        Output: 6

    Example 2:
        Input: 
            grid : [[0,0,1,1], [0,0,0,0], [1,1,1,1], [0,1,1,1]]
            bucket_capacity : 2
        Output: 5
    
    Example 3:
        Input: 
            grid : [[0,0,0], [0,0,0]]
            bucket_capacity : 5
        Output: 0

    Constraints:
        * all wells have the same length
        * 1 <= grid.length <= 10^2
        * 1 <= grid[:,1].length <= 10^2
        * grid[i][j] -> 0 | 1
        * 1 <= capacity <= 10
    """
    total_trips = 0
    for row in grid:
        water_units = sum(row)
        if water_units > 0:
            trips = math.ceil(water_units / capacity)
            total_trips += trips
    return total_trips
```
